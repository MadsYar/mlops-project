{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>This is the documentation for our project titled \"danish_to_english_llm\". It is part of the course called \"02476 Machine learning operations\" at the Technical University of Denmark.</p>"},{"location":"data/","title":"Data","text":""},{"location":"data/#danish_to_english_llm.data.TranslationDataset","title":"danish_to_english_llm.data.TranslationDataset","text":"<p>               Bases: <code>Dataset</code></p> <p>This class handles the downloading, loading and preprocessing of Danish and English translations of short sentences for training a T5 model.</p> <pre><code>Attributes:\n    mode (str): The dataset split using ('train', 'val', or 'test').\n    tokenizer (T5TokenizerFast): T5 tokenizer for processing text.\n    max_length (int): Maximum sequence length for tokenization.\n    data_dir (Path): Root directory for storing dataset files.\n    raw_dir (Path): Directory for raw downloaded data.\n    processed_dir (Path): Directory for processed and cached data.\n    data (List[Dict[str, str]]): Processed translation pairs.\n</code></pre> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the dataset.</p> <code>__len__</code> <p>Return length of dataset.</p> <code>__getitem__</code> <p>Get item from dataset.</p> Source code in <code>src/danish_to_english_llm/data.py</code> <pre><code>class TranslationDataset(Dataset):\n    \"\"\"\n    This class handles the downloading, loading and preprocessing of Danish and English\n    translations of short sentences for training a T5 model.\n\n        Attributes:\n            mode (str): The dataset split using ('train', 'val', or 'test').\n            tokenizer (T5TokenizerFast): T5 tokenizer for processing text.\n            max_length (int): Maximum sequence length for tokenization.\n            data_dir (Path): Root directory for storing dataset files.\n            raw_dir (Path): Directory for raw downloaded data.\n            processed_dir (Path): Directory for processed and cached data.\n            data (List[Dict[str, str]]): Processed translation pairs.\n    \"\"\"\n\n    def __init__(self, mode: str, tokenizer: T5TokenizerFast, max_length: int = 128, data_dir: str = \"data\") -&gt; None:\n        \"\"\"\n        Initialize the dataset.\n\n            Args:\n                mode: Dataset into three modes ('train', 'val', or 'test').\n                tokenizer: T5 tokenizer for processing text.\n                max_length: Maximum length for tokenization (default: 128).\n                data_dir: Root directory for storing dataset files.\n\n            Raises:\n                ValueError: If mode is not one of 'train', 'val', or 'test'.\n                RuntimeError: If dataset initialization fails.\n        \"\"\"\n        logger.info(f\"Initializing {mode} dataset\")\n\n        if mode not in [\"train\", \"val\", \"test\"]:\n            logger.error(f\"Invalid mode: {mode}\")\n            raise ValueError(\"Invalid mode. Please choose from 'train', 'val', or 'test'.\")\n\n        self.mode = mode\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.data_dir = Path(data_dir)\n\n        self.data: List[Dict[str, str]] = []\n\n        self.dataset = None\n\n        # Create directories if they don't exist\n        self.raw_dir = self.data_dir / \"raw\"\n        self.processed_dir = self.data_dir / \"processed\"\n        self.raw_dir.mkdir(parents=True, exist_ok=True)\n        self.processed_dir.mkdir(parents=True, exist_ok=True)\n\n        try:\n            # First check if processed data exists\n            if any(self.processed_dir.glob(\"*.pt\")):\n                logger.info(f\"Loading processed data for {mode} split...\")\n                loaded_data = torch.load(self.processed_dir / f\"{mode}.pt\", weights_only=True)\n                self.data = loaded_data\n                logger.success(f\"Loaded {len(self.data)} examples for {mode} split\")\n                return\n\n            # If no processed data, check raw data\n            raw_files = list(self.raw_dir.glob(\"*\"))\n            only_gitkeep = len(raw_files) == 1 and raw_files[0].name == \".gitkeep\"\n\n            # Download if necessary\n            if only_gitkeep:\n                logger.info(\"Raw directory empty. Downloading dataset...\")\n                self.dataset = self._download()\n            else:\n                logger.info(\"Loading existing raw data...\")\n                self.dataset = load_dataset(str(self.raw_dir))\n\n            # Preprocess\n            logger.info(\"Starting preprocessing...\")\n            self._preprocess()\n\n            # Load the processed data after preprocessing\n            logger.info(f\"Loading processed data for {mode} split...\")\n            loaded_data = torch.load(self.processed_dir / f\"{mode}.pt\", weights_only=True)\n            self.data = loaded_data\n            logger.success(f\"Loaded {len(self.data)} examples for {mode} split\")\n\n        except Exception as e:\n            logger.error(f\"Error initializing dataset: {str(e)}\")\n            raise RuntimeError(f\"Error initializing dataset: {str(e)}\")\n\n    def _download(self):\n        \"\"\"\n        Downloads the dataset from HuggingFace.\n\n            Returns:\n                Dataset: The downloaded HuggingFace dataset.\n\n            Raises:\n                RuntimeError: If dataset download fails.\n        \"\"\"\n\n        print(\"Downloading dataset from HuggingFace...\")\n        try:\n            dataset = load_dataset(\"kaitchup/opus-Danish-to-English\")\n            dataset.save_to_disk(self.raw_dir)\n            logger.success(\"Dataset downloaded successfully\")\n            return dataset\n        except Exception as e:\n            logger.error(f\"Failed to download dataset: {str(e)}\")\n            raise RuntimeError(f\"Failed to download dataset: {str(e)}\")\n\n    def _preprocess(self) -&gt; None:\n        \"\"\"\n        Preprocess the dataset and create train/val/test splits.\n\n            Raises:\n                ValueError: If self.dataset is None\n                RuntimeError: If preprocessing fails\n        \"\"\"\n\n        logger.info(\"Preprocessing dataset...\")\n\n        if self.dataset is None:\n            logger.error(\"No dataset loaded. self.dataset is None.\")\n            raise ValueError(\"No dataset loaded. self.dataset is None.\")\n\n        try:\n            # Process training data\n            train_data = self._process_split(self.dataset[\"train\"])\n            val_data = self._process_split(self.dataset[\"validation\"])\n\n            logger.info(f\"Processed {len(train_data)} training examples\")\n            logger.info(f\"Processed {len(val_data)} validation examples\")\n\n            # Create train/test split from training data\n            random.shuffle(train_data)\n            split_idx = int(0.95 * len(train_data))\n            final_train_data = train_data[:split_idx]\n            test_data = train_data[split_idx:]\n\n            # Save splits\n            torch.save(final_train_data, self.processed_dir / \"train.pt\")\n            torch.save(test_data, self.processed_dir / \"test.pt\")\n            torch.save(val_data, self.processed_dir / \"val.pt\")\n\n            logger.success(\n                f\"Saved {len(final_train_data)} training, {len(test_data)} test, and {len(val_data)} validation examples\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Error during preprocessing: {str(e)}\")\n            raise RuntimeError(f\"Error during preprocessing: {str(e)}\")\n\n    def _process_split(self, split_data) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Process a single data split.\n\n            Args:\n                split_data: Raw dataset split containing text pairs.\n\n            Returns:\n                List[Dict[str, str]]: List of dictionaries containing Danish-English pairs.\n        \"\"\"\n\n        processed_data = []\n        for item in split_data:\n            text = item[\"text\"]\n            if \"###&gt;\" in text:\n                danish, english = text.split(\"###&gt;\")\n                processed_data.append({\"danish\": danish.strip(), \"english\": english.strip()})\n        return processed_data\n\n    def _prepare_input(self, danish_text: str, english_text: str) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Tokenize and prepare input for the model.\n\n            Args:\n                danish_text: Source text in Danish.\n                english_text: Target text in English.\n\n            Returns:\n                Dict[str, torch.Tensor]: Dictionary containing tokenized input_ids,\n                    attention_mask, and labels.\n        \"\"\"\n\n        # Prepare input\n        source_encoding = self.tokenizer(\n            f\"translate Danish to English: {danish_text}\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        # Prepare target\n        target_encoding = self.tokenizer(\n            english_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": source_encoding.input_ids.squeeze(),\n            \"attention_mask\": source_encoding.attention_mask.squeeze(),\n            \"labels\": target_encoding.input_ids.squeeze(),\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return length of dataset.\"\"\"\n\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Get item from dataset.\n\n        Args:\n            idx: Index of the desired example.\n\n        Returns:\n            Dict[str, torch.Tensor]: Dictionary containing tokenized input_ids,\n                attention_mask, and labels.\n        \"\"\"\n\n        item = self.data[idx]\n        return self._prepare_input(item[\"danish\"], item[\"english\"])\n</code></pre>"},{"location":"data/#danish_to_english_llm.data.TranslationDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    mode: str,\n    tokenizer: T5TokenizerFast,\n    max_length: int = 128,\n    data_dir: str = \"data\",\n) -&gt; None\n</code></pre> <p>Initialize the dataset.</p> <pre><code>Args:\n    mode: Dataset into three modes ('train', 'val', or 'test').\n    tokenizer: T5 tokenizer for processing text.\n    max_length: Maximum length for tokenization (default: 128).\n    data_dir: Root directory for storing dataset files.\n\nRaises:\n    ValueError: If mode is not one of 'train', 'val', or 'test'.\n    RuntimeError: If dataset initialization fails.\n</code></pre> Source code in <code>src/danish_to_english_llm/data.py</code> <pre><code>def __init__(self, mode: str, tokenizer: T5TokenizerFast, max_length: int = 128, data_dir: str = \"data\") -&gt; None:\n    \"\"\"\n    Initialize the dataset.\n\n        Args:\n            mode: Dataset into three modes ('train', 'val', or 'test').\n            tokenizer: T5 tokenizer for processing text.\n            max_length: Maximum length for tokenization (default: 128).\n            data_dir: Root directory for storing dataset files.\n\n        Raises:\n            ValueError: If mode is not one of 'train', 'val', or 'test'.\n            RuntimeError: If dataset initialization fails.\n    \"\"\"\n    logger.info(f\"Initializing {mode} dataset\")\n\n    if mode not in [\"train\", \"val\", \"test\"]:\n        logger.error(f\"Invalid mode: {mode}\")\n        raise ValueError(\"Invalid mode. Please choose from 'train', 'val', or 'test'.\")\n\n    self.mode = mode\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.data_dir = Path(data_dir)\n\n    self.data: List[Dict[str, str]] = []\n\n    self.dataset = None\n\n    # Create directories if they don't exist\n    self.raw_dir = self.data_dir / \"raw\"\n    self.processed_dir = self.data_dir / \"processed\"\n    self.raw_dir.mkdir(parents=True, exist_ok=True)\n    self.processed_dir.mkdir(parents=True, exist_ok=True)\n\n    try:\n        # First check if processed data exists\n        if any(self.processed_dir.glob(\"*.pt\")):\n            logger.info(f\"Loading processed data for {mode} split...\")\n            loaded_data = torch.load(self.processed_dir / f\"{mode}.pt\", weights_only=True)\n            self.data = loaded_data\n            logger.success(f\"Loaded {len(self.data)} examples for {mode} split\")\n            return\n\n        # If no processed data, check raw data\n        raw_files = list(self.raw_dir.glob(\"*\"))\n        only_gitkeep = len(raw_files) == 1 and raw_files[0].name == \".gitkeep\"\n\n        # Download if necessary\n        if only_gitkeep:\n            logger.info(\"Raw directory empty. Downloading dataset...\")\n            self.dataset = self._download()\n        else:\n            logger.info(\"Loading existing raw data...\")\n            self.dataset = load_dataset(str(self.raw_dir))\n\n        # Preprocess\n        logger.info(\"Starting preprocessing...\")\n        self._preprocess()\n\n        # Load the processed data after preprocessing\n        logger.info(f\"Loading processed data for {mode} split...\")\n        loaded_data = torch.load(self.processed_dir / f\"{mode}.pt\", weights_only=True)\n        self.data = loaded_data\n        logger.success(f\"Loaded {len(self.data)} examples for {mode} split\")\n\n    except Exception as e:\n        logger.error(f\"Error initializing dataset: {str(e)}\")\n        raise RuntimeError(f\"Error initializing dataset: {str(e)}\")\n</code></pre>"},{"location":"data/#danish_to_english_llm.data.TranslationDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return length of dataset.</p> Source code in <code>src/danish_to_english_llm/data.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return length of dataset.\"\"\"\n\n    return len(self.data)\n</code></pre>"},{"location":"data/#danish_to_english_llm.data.TranslationDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; Dict[str, torch.Tensor]\n</code></pre> <p>Get item from dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the desired example.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dict[str, torch.Tensor]: Dictionary containing tokenized input_ids, attention_mask, and labels.</p> Source code in <code>src/danish_to_english_llm/data.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Get item from dataset.\n\n    Args:\n        idx: Index of the desired example.\n\n    Returns:\n        Dict[str, torch.Tensor]: Dictionary containing tokenized input_ids,\n            attention_mask, and labels.\n    \"\"\"\n\n    item = self.data[idx]\n    return self._prepare_input(item[\"danish\"], item[\"english\"])\n</code></pre>"},{"location":"data/#danish_to_english_llm.data.get_dataloaders","title":"danish_to_english_llm.data.get_dataloaders","text":"<pre><code>get_dataloaders(\n    tokenizer: T5TokenizerFast,\n    batch_size: int = 16,\n    max_length: int = 128,\n    num_workers: int = 4,\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]\n</code></pre> <p>Create DataLoaders for training, validation and testing.</p> <pre><code>Args:\n    tokenizer: T5 tokenizer for processing text.\n    batch_size: Batch size for training (default: 16).\n    max_length: Maximum sequence length (default: 128).\n    num_workers: Number of worker processes for data loading (default: 4).\n\nReturns:\n    Tuple[DataLoader, DataLoader, DataLoader]: Training, validation, and test\n        data loaders.\n</code></pre> Source code in <code>src/danish_to_english_llm/data.py</code> <pre><code>def get_dataloaders(\n    tokenizer: T5TokenizerFast, batch_size: int = 16, max_length: int = 128, num_workers: int = 4\n) -&gt; Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Create DataLoaders for training, validation and testing.\n\n        Args:\n            tokenizer: T5 tokenizer for processing text.\n            batch_size: Batch size for training (default: 16).\n            max_length: Maximum sequence length (default: 128).\n            num_workers: Number of worker processes for data loading (default: 4).\n\n        Returns:\n            Tuple[DataLoader, DataLoader, DataLoader]: Training, validation, and test\n                data loaders.\n    \"\"\"\n\n    train_dataset = TranslationDataset(\"train\", tokenizer, max_length)\n    val_dataset = TranslationDataset(\"val\", tokenizer, max_length)\n    test_dataset = TranslationDataset(\"test\", tokenizer, max_length)\n\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n    )\n\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"data/#danish_to_english_llm.data.preprocess","title":"danish_to_english_llm.data.preprocess","text":"<pre><code>preprocess(mode: str) -&gt; TranslationDataset\n</code></pre> <p>Preprocess the dataset for a given mode.</p> <pre><code>Args:\n    mode: Dataset split to preprocess ('train', 'val', or 'test').\n\nReturns:\n    TranslationDataset: Preprocessed dataset for the specified mode.\n</code></pre> Source code in <code>src/danish_to_english_llm/data.py</code> <pre><code>def preprocess(mode: str) -&gt; TranslationDataset:\n    \"\"\"\n    Preprocess the dataset for a given mode.\n\n        Args:\n            mode: Dataset split to preprocess ('train', 'val', or 'test').\n\n        Returns:\n            TranslationDataset: Preprocessed dataset for the specified mode.\n    \"\"\"\n\n    dataset = TranslationDataset(mode, T5TokenizerFast.from_pretrained(\"google-t5/t5-small\"))\n    return dataset\n</code></pre>"},{"location":"model/","title":"Model","text":""},{"location":"model/#danish_to_english_llm.model.T5LightningModel","title":"danish_to_english_llm.model.T5LightningModel","text":"<p>               Bases: <code>LightningModule</code></p> <p>Implementation of a T5 model for danish-english translation using PyTorch Lightning.</p> <p>Attributes:</p> Name Type Description <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>max_length</code> <code>int</code> <p>Maximum length for inputs and outputs.</p> <code>model</code> <code>T5ForConditionalGeneration</code> <p>The underlying T5 model.</p> <code>tokenizer</code> <code>T5TokenizerFast</code> <p>T5 tokenizer for processing text.</p> <code>train_losses</code> <code>List[float]</code> <p>List of training losses.</p> <code>val_losses</code> <code>List[float]</code> <p>List of validation losses.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the T5 Lightning model.</p> <code>forward</code> <p>Forward pass of the model.</p> <code>training_step</code> <p>Performs a training step.</p> <code>validation_step</code> <p>Performs a validation step.</p> <code>configure_optimizers</code> <p>Configure optimizer and learning rate scheduler.</p> <code>get_metrics</code> <p>Get training metrics.</p> <code>translate</code> <p>Translates Danish text to English.</p> <code>load_from_checkpoint</code> <p>Load a model from a checkpoint.</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>class T5LightningModel(pl.LightningModule):\n    \"\"\"\n    Implementation of a T5 model for danish-english translation using PyTorch Lightning.\n\n    Attributes:\n        learning_rate (float): Learning rate for the optimizer.\n        max_length (int): Maximum length for inputs and outputs.\n        model (T5ForConditionalGeneration): The underlying T5 model.\n        tokenizer (T5TokenizerFast): T5 tokenizer for processing text.\n        train_losses (List[float]): List of training losses.\n        val_losses (List[float]): List of validation losses.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained_model: str = \"google-t5/t5-small\",\n        learning_rate: float = 1e-4,\n        max_length: int = 128,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the T5 Lightning model.\n\n        Args:\n            pretrained_model: Path for the pretrained T5 model.\n            learning_rate: Learning rate for the optimizer.\n            max_length: Maximum sequence length for tokenization.\n        \"\"\"\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.max_length = max_length\n\n        # Load pretrained model and tokenizer\n        self.model = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n        self.tokenizer = T5TokenizerFast.from_pretrained(pretrained_model)\n\n        # Initialize metrics\n        self.train_losses: List[float] = []\n        self.val_losses: List[float] = []\n\n        # Save hyperparameters\n        self.save_hyperparameters()\n\n    def forward(\n        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            input_ids: Tensor of input token IDs.\n            attention_mask: Tensor indicating which tokens should be attended to.\n            labels: Optional tensor of target token IDs for training.\n\n        Returns:\n            torch.Tensor: Model outputs containing loss and logits.\n        \"\"\"\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        return outputs\n\n    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a training step.\n\n        Args:\n            batch: Dictionary containing input_ids, attention_mask, and labels.\n            batch_idx: Index of the current batch.\n\n        Returns:\n            torch.Tensor: Computed loss for backpropagation.\n        \"\"\"\n        outputs = self(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n\n        loss = outputs.loss\n        self.train_losses.append(loss.item())\n\n        # Log training loss\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n\n        return loss\n\n    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -&gt; None:\n        \"\"\"\n        Performs a validation step.\n\n        Args:\n            batch: Dictionary containing input_ids, attention_mask, and labels.\n            batch_idx: Index of the current batch.\n        \"\"\"\n        outputs = self(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n\n        loss = outputs.loss\n        self.val_losses.append(loss.item())\n\n        # Log validation loss\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure optimizer and learning rate scheduler.\n\n        Returns:\n            Dict: Configuration dictionary containing optimizer (AdamW) and scheduler (OneCycleLR).\n        \"\"\"\n        # Prepare optimizer\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n        # Add linear scheduler with warmup\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=self.learning_rate,\n            steps_per_epoch=self.trainer.estimated_stepping_batches,\n            epochs=self.trainer.max_epochs,\n            pct_start=0.1,\n        )\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n\n    def get_metrics(self):\n        \"\"\"\n        Get training metrics.\n\n        Returns:\n            Dict: Dictionary containing training and validation losses.\n        \"\"\"\n        return {\"train_loss\": self.train_losses, \"val_loss\": self.val_losses}\n\n    def translate(self, danish_text: str) -&gt; str:\n        \"\"\"\n        Translates Danish text to English.\n\n        Args:\n            danish_text: Input text in Danish.\n\n        Returns:\n            str: Translated text in English.\n        \"\"\"\n        # Prepare input\n        inputs = self.tokenizer(\n            f\"translate Danish to English: {danish_text}\",\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        # Move inputs to device\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # Generate translation\n        outputs = self.model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=self.max_length,\n            num_beams=4,\n            length_penalty=1.0,\n            early_stopping=True,\n        )\n\n        # Decode and return\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    def load_from_checkpoint(checkpoint_path: str) -&gt; \"T5LightningModel\":\n        \"\"\"\n        Load a model from a checkpoint.\n\n        Args:\n            checkpoint_path: Path to the checkpoint file.\n\n        Returns:\n            T5LightningModel: Loaded model.\n        \"\"\"\n        # Load model and return\n        model = T5LightningModel()\n        model.load_state_dict(torch.load(checkpoint_path, map_location=model.device))\n        model.eval()\n        return model\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    pretrained_model: str = \"google-t5/t5-small\",\n    learning_rate: float = 0.0001,\n    max_length: int = 128,\n) -&gt; None\n</code></pre> <p>Initialize the T5 Lightning model.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model</code> <code>str</code> <p>Path for the pretrained T5 model.</p> <code>'google-t5/t5-small'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.0001</code> <code>max_length</code> <code>int</code> <p>Maximum sequence length for tokenization.</p> <code>128</code> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def __init__(\n    self,\n    pretrained_model: str = \"google-t5/t5-small\",\n    learning_rate: float = 1e-4,\n    max_length: int = 128,\n) -&gt; None:\n    \"\"\"\n    Initialize the T5 Lightning model.\n\n    Args:\n        pretrained_model: Path for the pretrained T5 model.\n        learning_rate: Learning rate for the optimizer.\n        max_length: Maximum sequence length for tokenization.\n    \"\"\"\n    super().__init__()\n    self.learning_rate = learning_rate\n    self.max_length = max_length\n\n    # Load pretrained model and tokenizer\n    self.model = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n    self.tokenizer = T5TokenizerFast.from_pretrained(pretrained_model)\n\n    # Initialize metrics\n    self.train_losses: List[float] = []\n    self.val_losses: List[float] = []\n\n    # Save hyperparameters\n    self.save_hyperparameters()\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    labels: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor indicating which tokens should be attended to.</p> required <code>labels</code> <code>Optional[Tensor]</code> <p>Optional tensor of target token IDs for training.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Model outputs containing loss and logits.</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def forward(\n    self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        input_ids: Tensor of input token IDs.\n        attention_mask: Tensor indicating which tokens should be attended to.\n        labels: Optional tensor of target token IDs for training.\n\n    Returns:\n        torch.Tensor: Model outputs containing loss and logits.\n    \"\"\"\n    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n    return outputs\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.training_step","title":"training_step","text":"<pre><code>training_step(\n    batch: Dict[str, torch.Tensor], batch_idx: int\n) -&gt; torch.Tensor\n</code></pre> <p>Performs a training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>Dictionary containing input_ids, attention_mask, and labels.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed loss for backpropagation.</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a training step.\n\n    Args:\n        batch: Dictionary containing input_ids, attention_mask, and labels.\n        batch_idx: Index of the current batch.\n\n    Returns:\n        torch.Tensor: Computed loss for backpropagation.\n    \"\"\"\n    outputs = self(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n\n    loss = outputs.loss\n    self.train_losses.append(loss.item())\n\n    # Log training loss\n    self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n\n    return loss\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.validation_step","title":"validation_step","text":"<pre><code>validation_step(\n    batch: Dict[str, torch.Tensor], batch_idx: int\n) -&gt; None\n</code></pre> <p>Performs a validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>Dictionary containing input_ids, attention_mask, and labels.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch.</p> required Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -&gt; None:\n    \"\"\"\n    Performs a validation step.\n\n    Args:\n        batch: Dictionary containing input_ids, attention_mask, and labels.\n        batch_idx: Index of the current batch.\n    \"\"\"\n    outputs = self(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n\n    loss = outputs.loss\n    self.val_losses.append(loss.item())\n\n    # Log validation loss\n    self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer and learning rate scheduler.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>Configuration dictionary containing optimizer (AdamW) and scheduler (OneCycleLR).</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"\n    Configure optimizer and learning rate scheduler.\n\n    Returns:\n        Dict: Configuration dictionary containing optimizer (AdamW) and scheduler (OneCycleLR).\n    \"\"\"\n    # Prepare optimizer\n    optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    # Add linear scheduler with warmup\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=self.learning_rate,\n        steps_per_epoch=self.trainer.estimated_stepping_batches,\n        epochs=self.trainer.max_epochs,\n        pct_start=0.1,\n    )\n\n    return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics()\n</code></pre> <p>Get training metrics.</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>Dictionary containing training and validation losses.</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def get_metrics(self):\n    \"\"\"\n    Get training metrics.\n\n    Returns:\n        Dict: Dictionary containing training and validation losses.\n    \"\"\"\n    return {\"train_loss\": self.train_losses, \"val_loss\": self.val_losses}\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.translate","title":"translate","text":"<pre><code>translate(danish_text: str) -&gt; str\n</code></pre> <p>Translates Danish text to English.</p> <p>Parameters:</p> Name Type Description Default <code>danish_text</code> <code>str</code> <p>Input text in Danish.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Translated text in English.</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def translate(self, danish_text: str) -&gt; str:\n    \"\"\"\n    Translates Danish text to English.\n\n    Args:\n        danish_text: Input text in Danish.\n\n    Returns:\n        str: Translated text in English.\n    \"\"\"\n    # Prepare input\n    inputs = self.tokenizer(\n        f\"translate Danish to English: {danish_text}\",\n        max_length=self.max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n\n    # Move inputs to device\n    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n    # Generate translation\n    outputs = self.model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=self.max_length,\n        num_beams=4,\n        length_penalty=1.0,\n        early_stopping=True,\n    )\n\n    # Decode and return\n    return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n</code></pre>"},{"location":"model/#danish_to_english_llm.model.T5LightningModel.load_from_checkpoint","title":"load_from_checkpoint","text":"<pre><code>load_from_checkpoint(\n    checkpoint_path: str,\n) -&gt; T5LightningModel\n</code></pre> <p>Load a model from a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Path to the checkpoint file.</p> required <p>Returns:</p> Name Type Description <code>T5LightningModel</code> <code>T5LightningModel</code> <p>Loaded model.</p> Source code in <code>src/danish_to_english_llm/model.py</code> <pre><code>def load_from_checkpoint(checkpoint_path: str) -&gt; \"T5LightningModel\":\n    \"\"\"\n    Load a model from a checkpoint.\n\n    Args:\n        checkpoint_path: Path to the checkpoint file.\n\n    Returns:\n        T5LightningModel: Loaded model.\n    \"\"\"\n    # Load model and return\n    model = T5LightningModel()\n    model.load_state_dict(torch.load(checkpoint_path, map_location=model.device))\n    model.eval()\n    return model\n</code></pre>"},{"location":"train/","title":"Train","text":""},{"location":"train/#danish_to_english_llm.train.train","title":"danish_to_english_llm.train.train","text":"<pre><code>train(config) -&gt; None\n</code></pre> <p>Trains the translation model for Danish to English translation.</p> What this training pipeline does <ul> <li>Loading data</li> <li>Initializing model</li> <li>Training configuration</li> <li>Wandb logging</li> <li>Training the model</li> <li>Saving the model</li> <li>Plotting training statistics</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>Hydra configuration object.</p> required Source code in <code>src/danish_to_english_llm/train.py</code> <pre><code>@hydra.main(version_base=None, config_path=\"../../configs\", config_name=\"config.yaml\")\ndef train(config) -&gt; None:\n    \"\"\"\n    Trains the translation model for Danish to English translation.\n\n    What this training pipeline does:\n        - Loading data\n        - Initializing model\n        - Training configuration\n        - Wandb logging\n        - Training the model\n        - Saving the model\n        - Plotting training statistics\n\n    Args:\n        config: Hydra configuration object.\n    \"\"\"\n\n    # Initialize tokenizer and get dataloaders\n    logger.info(\"Starting training\")\n    logger.info(f\"Configuration: \\n {OmegaConf.to_yaml(config)}\")\n    train_loader, val_loader, _ = get_dataloaders(\n        tokenizer=T5TokenizerFast.from_pretrained(config.experiment.training.model_name),\n        batch_size=config.experiment.training.batch_size,\n        max_length=config.experiment.training.max_length,\n        num_workers=config.experiment.training.num_workers,\n    )\n\n    run = wandb.init(\n        project=\"Danish-to-English\",\n        config={\n            \"lr\": config.experiment.training.learning_rate,\n            \"batch_size\": config.experiment.training.batch_size,\n            \"epochs\": config.experiment.training.max_epochs,\n        },\n    )\n\n    # Initialize model\n    logger.info(\"Loading model\")\n    model = T5LightningModel(\n        pretrained_model=config.experiment.training.model_name,\n        learning_rate=config.experiment.training.learning_rate,\n        max_length=config.experiment.training.max_length,\n    )\n    logger.success(\"Model loaded successfully\")\n\n    # Setup callbacks\n    logger.info(\"Setting up callbacks\")\n    callbacks = [\n        EarlyStopping(\n            monitor=config.experiment.callbacks.monitor,\n            patience=config.experiment.callbacks.patience,\n            verbose=True,\n            mode=config.experiment.callbacks.mode,\n        ),\n        ModelCheckpoint(\n            dirpath=config.experiment.callbacks.dirpath,\n            monitor=config.experiment.callbacks.monitor,\n            mode=config.experiment.callbacks.mode,\n            filename=config.experiment.callbacks.filename,\n            save_top_k=config.experiment.callbacks.save_top_k,\n            verbose=True,\n        ),\n    ]\n    logger.success(\"Callback are set up successfully\")\n\n    # Initialize wandb logger\n    logger.info(\"Initializing wandb logger\")\n    wandb_logger = WandbLogger(project=\"t5-training\", name=\"t5-small\")\n    logger.success(\"Wandb logger initialized successfully\")\n\n    # Setup trainer\n    logger.info(\"Setting up trainer\")\n    trainer = pl.Trainer(\n        max_epochs=config.experiment.training.max_epochs,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        default_root_dir=\"./lightning_logs\",  # TODO: Set this to a proper directory\n        accelerator=config.experiment.trainer.accelerator,\n        devices=config.experiment.trainer.devices,\n        gradient_clip_val=config.experiment.trainer.gradient_clip_val,\n        precision=config.experiment.trainer.precision,\n        accumulate_grad_batches=config.experiment.trainer.accumulate_grad_batches,\n    )\n    logger.success(\"Trainer is set up successfully\")\n\n    # Train model\n    logger.info(\"Start training the model\")\n    trainer.fit(model, train_loader, val_loader)\n    logger.success(\"Training finished\")\n\n    # Save model\n    torch.save(model.state_dict(), \"models/final_model.pth\")  # TODO: Find a better naming scheme?\n    artifact = wandb.Artifact(\n        name=\"Danish-to-English-model\", type=\"model\", description=\"A model trained to translate Danish to English\"\n    )\n\n    artifact.add_file(\"models/final_model.pth\")\n    run.log_artifact(artifact)\n    logger.success(\"Model saved\")\n\n    # Get metrics and plot\n    logger.info(\"Plotting training statistics\")\n    metrics = model.get_metrics()\n\n    # Plot training statistics\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Plot training loss\n    axs[0].plot(metrics[\"train_loss\"])\n    axs[0].set_title(\"Training Loss\")\n    axs[0].set_xlabel(\"Step\")\n    axs[0].set_ylabel(\"Loss\")\n\n    # Plot validation loss\n    axs[1].plot(metrics[\"val_loss\"])\n    axs[1].set_title(\"Validation Loss\")\n    axs[1].set_xlabel(\"Step\")\n    axs[1].set_ylabel(\"Loss\")\n\n    plt.tight_layout()\n    fig.savefig(\"reports/figures/training_statistics.png\")\n    plt.close()\n    logger.success(\"Training script is all done\")\n</code></pre>"}]}